"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4008],{3905:(e,t,n)=>{n.d(t,{Zo:()=>l,kt:()=>d});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=a.createContext({}),p=function(e){var t=a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},l=function(e){var t=p(e.components);return a.createElement(c.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,l=o(e,["components","mdxType","originalType","parentName"]),u=p(n),h=r,d=u["".concat(c,".").concat(h)]||u[h]||m[h]||i;return n?a.createElement(d,s(s({ref:t},l),{},{components:n})):a.createElement(d,s({ref:t},l))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,s=new Array(i);s[0]=h;var o={};for(var c in t)hasOwnProperty.call(t,c)&&(o[c]=t[c]);o.originalType=e,o[u]="string"==typeof e?e:r,s[1]=o;for(var p=2;p<i;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},8002:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const i={},s="Map-Reduce Chains",o={unversionedId:"chains/map-reduce-chains",id:"chains/map-reduce-chains",title:"Map-Reduce Chains",description:'Map-Reduce chains are a powerful way to process large amounts of text using large language models (LLMs). They consist of two main steps: a "map" step, which processes each text chunk independently, and a "reduce" step, which combines the results of the map step into a single output. This approach enables the efficient processing of large documents that exceed the LLM\'s context window size.',source:"@site/docs/chains/02-map-reduce-chains.md",sourceDirName:"chains",slug:"/chains/map-reduce-chains",permalink:"/docs/chains/map-reduce-chains",draft:!1,editUrl:"https://github.com/sobelio/llm-chain/tree/main/docs/docs/chains/02-map-reduce-chains.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"sidebar",previous:{title:"sequential-chains",permalink:"/docs/chains/sequential-chains"},next:{title:"Conversational Chains",permalink:"/docs/chains/conversational"}},c={},p=[],l={toc:p},u="wrapper";function m(e){let{components:t,...n}=e;return(0,r.kt)(u,(0,a.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"map-reduce-chains"},"Map-Reduce Chains"),(0,r.kt)("p",null,'Map-Reduce chains are a powerful way to process large amounts of text using large language models (LLMs). They consist of two main steps: a "map" step, which processes each text chunk independently, and a "reduce" step, which combines the results of the map step into a single output. This approach enables the efficient processing of large documents that exceed the LLM\'s context window size.'),(0,r.kt)("p",null,"In this guide, we'll explain how to create and execute a map-reduce chain using an example. The example demonstrates how to summarize a Wikipedia article into bullet points using a two-step process:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},'The "map" step summarizes each chunk of the article into bullet points.'),(0,r.kt)("li",{parentName:"ol"},'The "reduce" step combines all bullet point summaries into a single summary.')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-rust"},'use llm_chain::chains::map_reduce::Chain;\nuse llm_chain::step::Step;\nuse llm_chain::{executor, parameters, prompt, Parameters};\n\n#[tokio::main(flavor = "current_thread")]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Create a new ChatGPT executor with the default settings\n    let exec = executor!()?;\n\n    // Create the "map" step to summarize an article into bullet points\n    let map_prompt = Step::for_prompt_template(prompt!(\n        "You are a bot for summarizing wikipedia articles, you are terse and focus on accuracy",\n        "Summarize this article into bullet points:\\n{{text}}"\n    ));\n\n    // Create the "reduce" step to combine multiple summaries into one\n    let reduce_prompt = Step::for_prompt_template(prompt!(\n        "You are a diligent bot that summarizes text",\n        "Please combine the articles below into one summary as bullet points:\\n{{text}}"\n    ));\n\n    // Create a map-reduce chain with the map and reduce steps\n    let chain = Chain::new(map_prompt, reduce_prompt);\n\n    // Load the content of the article to be summarized\n    let article = include_str!("article_to_summarize.md");\n\n    // Create a vector with the Parameters object containing the text of the article\n    let docs = vec![parameters!(article)];\n\n    // Run the chain with the provided documents and an empty Parameters object for the "reduce" step\n    let res = chain.run(docs, Parameters::new(), &exec).await.unwrap();\n\n    // Print the result to the console\n    println!("{}", res);\n    Ok(())\n}\n')),(0,r.kt)("p",null,"In this example, we start by importing the necessary modules and defining the main function. We then create a new ChatGPT executor using the executor!() macro."),(0,r.kt)("p",null,'Next, we create the "map" and "reduce" steps using Step::for_prompt_template(). The "map" step is responsible for summarizing each article chunk, while the "reduce" step combines the summaries into a single output.'),(0,r.kt)("p",null,'After defining the steps, we create a new Chain object by passing in the "map" and "reduce" steps. We then load the content of the article to be summarized and create a Parameters object containing the text.'),(0,r.kt)("p",null,'Finally, we execute the map-reduce chain using the chain.run() method, passing in the documents, an empty Parameters object for the "reduce" step, and the executor. The result is printed to the console.'),(0,r.kt)("p",null,"Map-Reduce chains offer an effective way to handle large documents or multiple documents using LLMs. By breaking the text into manageable chunks and combining the results, you can create efficient pipelines for text processing tasks such as summarization, translation, and analysis."))}m.isMDXComponent=!0}}]);