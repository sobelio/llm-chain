use super::*;
use serde::{Deserialize, Serialize};

pub type PromptWithFunctionCalls = prompt::ChatMessageCollection<FunctionCallBody>;

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatCompletionFunctions {
    /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    pub name: String,
    /// The description of what the function does.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// The parameters the functions accepts, described as a JSON Schema object.
    /// See the [guide](https://platform.openai.com/docs/guides/gpt/function-calling) for examples,
    /// and the [JSON Schema](https://json-schema.org/understanding-json-schema/) reference for documentation about the format.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parameters: Option<serde_json::Value>,
}

impl From<ChatCompletionFunctions> for async_openai::types::ChatCompletionFunctions {
    fn from(functions: ChatCompletionFunctions) -> Self {
        Self {
            name: functions.name,
            description: functions.description,
            parameters: functions.parameters,
        }
    }
}

impl From<async_openai::types::ChatCompletionFunctions> for ChatCompletionFunctions {
    fn from(functions: async_openai::types::ChatCompletionFunctions) -> Self {
        Self {
            name: functions.name,
            description: functions.description,
            parameters: functions.parameters,
        }
    }
}

/// The name and arguments of a function that should be called, as generated by the model.
#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct FunctionCall {
    /// The name of the function to call.
    pub name: String,
    /// The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    pub arguments: String,
}

impl From<FunctionCall> for async_openai::types::FunctionCall {
    fn from(call: FunctionCall) -> Self {
        Self {
            name: call.name,
            arguments: call.arguments,
        }
    }
}

impl From<async_openai::types::FunctionCall> for FunctionCall {
    fn from(call: async_openai::types::FunctionCall) -> Self {
        Self {
            name: call.name,
            arguments: call.arguments,
        }
    }
}

#[derive(Debug, Serialize, Deserialize, Default, Clone, PartialEq)]
pub struct FunctionCallBody {
    /// The contents of the message.
    /// `content` is required for all messages except assistant messages with function calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<String>,
    /// The name of the author of this message. `name` is required if role is function,
    /// and it should be the name of the function whose response is in the `content`.
    /// May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The name and arguments of a function that should be called, as generated by the model.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub function_call: Option<FunctionCall>,
}

#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]
#[serde(untagged)]
pub enum ChatCompletionFunctionCall {
    String(String),
    Object(serde_json::Value),
}

impl From<ChatCompletionFunctionCall> for async_openai::types::ChatCompletionFunctionCall {
    fn from(call: ChatCompletionFunctionCall) -> Self {
        match call {
            ChatCompletionFunctionCall::String(s) => Self::String(s),
            ChatCompletionFunctionCall::Object(o) => Self::Object(o),
        }
    }
}

impl From<async_openai::types::ChatCompletionFunctionCall> for ChatCompletionFunctionCall {
    fn from(call: async_openai::types::ChatCompletionFunctionCall) -> Self {
        match call {
            async_openai::types::ChatCompletionFunctionCall::String(s) => Self::String(s),
            async_openai::types::ChatCompletionFunctionCall::Object(o) => Self::Object(o),
        }
    }
}

fn format_chat_message(
    message: &prompt::ChatMessage<FunctionCallBody>,
) -> Result<ChatCompletionRequestMessage, StringTemplateError> {
    let role = convert_role(message.role());
    let content = message.body().content.clone();
    let name = message.body().name.clone();
    let function_call = message.body().function_call.clone().map(|v| v.into());
    Ok(ChatCompletionRequestMessage {
        role,
        content,
        name,
        function_call,
    })
}

pub fn format_chat_messages(
    messages: PromptWithFunctionCalls,
) -> Result<Vec<ChatCompletionRequestMessage>, StringTemplateError> {
    messages.iter().map(format_chat_message).collect()
}

pub fn create_chat_completion_with_function_call_request(
    model: String,
    prompt: &PromptWithFunctionCalls,
    functions: Vec<ChatCompletionFunctions>,
    function_call: ChatCompletionFunctionCall,
    is_streaming: bool,
) -> Result<CreateChatCompletionRequest, StringTemplateError> {
    let messages = format_chat_messages(prompt.clone())?;
    Ok(CreateChatCompletionRequest {
        model,
        messages,
        functions: Some(functions.into_iter().map(Into::into).collect()),
        function_call: Some(function_call.into()),
        temperature: None,
        top_p: None,
        n: Some(1),
        stream: Some(is_streaming),
        stop: None,
        max_tokens: None, // We should consider something here
        presence_penalty: None,
        frequency_penalty: None,
        logit_bias: None,
        user: None,
    })
}
